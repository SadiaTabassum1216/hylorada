{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# HyLoRADA: Hybrid Low-Rank Adaptation with Position-Adaptive Landmarks\n",
                "\n",
                "**Validated Configuration**: rsLoRA + Position Bias + Position-Adaptive Landmarks  \n",
                "**Improvement**: +18.37% PPL reduction (69.00 â†’ 56.33) on WikiText-2  \n",
                "**Parameters**: ~3.54M trainable (2.9% of GPT-2)\n",
                "\n",
                "## Quick Start for Kaggle\n",
                "\n",
                "1. Set accelerator to **GPU** (Settings > Accelerator > GPU)\n",
                "2. Run cells in order\n",
                "3. For comprehensive ablation, run `test_ablation_proper.py`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Disable tokenizers parallelism to avoid fork warnings\n",
                "import os\n",
                "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repo (Kaggle)\n",
                "import os\n",
                "if os.path.exists('hylorada'):\n",
                "    %cd hylorada\n",
                "    !git pull\n",
                "    print(\"âš ï¸ Repo updated! Please RESTART KERNEL (Runtime > Restart Session) to reload modules\")\n",
                "else:\n",
                "    !git clone https://github.com/SadiaTabassum1216/hylorada.git\n",
                "    %cd hylorada"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q transformers datasets accelerate tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "import torch\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Quick Demo - HyLoRADA Components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from hylorada import HyLoRADAConfig, HyLoRADAModel, LandmarkLoRA\n",
                "\n",
                "# Test Position-Adaptive LandmarkLoRA\n",
                "lm = LandmarkLoRA(\n",
                "    hidden_size=768, \n",
                "    num_landmarks=8,\n",
                "    max_positions=32768,\n",
                "    num_buckets=32\n",
                ")\n",
                "x = torch.randn(1, 64, 768)\n",
                "y = lm(x)\n",
                "print(f\"Position-Adaptive LandmarkLoRA: {x.shape} -> {y.shape}\")\n",
                "print(f\"Landmark params: {sum(p.numel() for p in lm.parameters()):,}\")\n",
                "print(f\"Architecture: {lm}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load base model (GPT-2)\n",
                "model_name = \"openai-community/gpt2\"\n",
                "base_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "# âœ… VALIDATED CONFIGURATION (Recommended)\n",
                "config = HyLoRADAConfig(\n",
                "    lora_rank=16,                 # Validated rank\n",
                "    lora_alpha=16.0,\n",
                "    use_dora_magnitude=False,     # âœ… Disabled (causes -5.62% degradation)\n",
                "    landmark_enabled=True,        # âœ… Enabled (+18.37% improvement, best result)\n",
                "    num_landmarks=8,\n",
                "    num_position_buckets=32,      # Position bucketing for landmarks\n",
                "    position_bias_enabled=True,   # âœ… Enabled (+2.11% improvement, 64 params)\n",
                "    # Long Context Features (Optional - not yet validated)\n",
                "    s2_attn_enabled=False,        # Set True for SÂ²-Attn (shifted sparse)\n",
                "    s2_group_size=2048,\n",
                "    rope_scaling_type=None,       # Set \"linear\" for RoPE scaling\n",
                ")\n",
                "\n",
                "print(\"âœ… Validated Configuration:\")\n",
                "print(f\"  - rsLoRA: +17.07% improvement\")\n",
                "print(f\"  - Position Bias: +2.11% (64 params)\")\n",
                "print(f\"  - Position-Adaptive Landmarks: +18.37% total (12.5K params)\")\n",
                "print(f\"  - DoRA: Disabled (causes degradation)\")\n",
                "print(f\"\\nExpected: +18.37% improvement (69.00 â†’ 56.33 PPL)\")\n",
                "print(\"\\nComponents:\", config.get_component_status())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply HyLoRADA to base model\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "# Move base model to device first\n",
                "base_model = base_model.to(device)\n",
                "\n",
                "# Apply HyLoRADA wrapper\n",
                "model = HyLoRADAModel(base_model, config)\n",
                "\n",
                "# Move entire model to device\n",
                "model = model.to(device)\n",
                "\n",
                "model.print_trainable_params()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Validation - Ablation Study\n",
                "\n",
                "Run the comprehensive ablation test to validate all components:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python test_ablation_proper.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Example (Optional)\n",
                "\n",
                "Train HyLoRADA on a small dataset:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training example (requires GPU)\n",
                "from hylorada.trainer import HyLoRADATrainer, TrainingConfig, create_long_context_dataloader\n",
                "from datasets import load_dataset\n",
                "\n",
                "# Load training data\n",
                "train_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
                "train_texts = [text.strip() for text in train_dataset[\"text\"] \n",
                "               if text.strip() and len(text.strip()) > 100][:500]\n",
                "\n",
                "# Create dataloader\n",
                "train_dataloader = create_long_context_dataloader(\n",
                "    dataset=train_texts,\n",
                "    tokenizer=tokenizer,\n",
                "    max_length=512,\n",
                "    batch_size=4,\n",
                ")\n",
                "\n",
                "# Training config\n",
                "train_config = TrainingConfig(\n",
                "    num_epochs=1,\n",
                "    learning_rate=1e-4,\n",
                "    per_device_batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    mixed_precision=\"bf16\",\n",
                "    output_dir=\"./output/hylorada_run\",\n",
                "    logging_steps=10,\n",
                ")\n",
                "\n",
                "# Create trainer\n",
                "trainer = HyLoRADATrainer(\n",
                "    model=model,\n",
                "    train_dataloader=train_dataloader,\n",
                "    config=train_config,\n",
                ")\n",
                "\n",
                "# Train\n",
                "print(\"ðŸš€ Starting training...\")\n",
                "trainer.train()\n",
                "print(\"\\nâœ… Training complete! Model saved to ./output/hylorada_run\")"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "gpu",
            "isGpuEnabled": true,
            "isInternetEnabled": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
