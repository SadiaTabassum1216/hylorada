{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# HyLoRADA: Hybrid Low-Rank Adaptation with Landmark Memory\n",
                "\n",
                "**Components**: rsLoRA + DoRA + LandmarkLoRA + PositionBias\n",
                "\n",
                "| Component | Description | Params |\n",
                "|-----------|-------------|--------|\n",
                "| rsLoRA | α/√r scaling | 0 |\n",
                "| DoRA | Magnitude normalization | ~86K |\n",
                "| LandmarkLoRA | Context summaries (Novel) | ~14K |\n",
                "| PositionBias | Position awareness | 64 |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repo (Kaggle)\n",
                "import os\n",
                "if os.path.exists('hylorada'):\n",
                "    %cd hylorada\n",
                "    !git pull\n",
                "else:\n",
                "    !git clone https://github.com/SadiaTabassum1216/hylorada.git\n",
                "    %cd hylorada"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q transformers datasets accelerate tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "import torch\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Quick Demo - HyLoRADA Components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from hylorada import HyLoRADAConfig, HyLoRADAModel, LandmarkLoRA\n",
                "\n",
                "# Test LandmarkLoRA (Novel component)\n",
                "lm = LandmarkLoRA(hidden_size=768, num_landmarks=8)\n",
                "x = torch.randn(1, 64, 768)\n",
                "y = lm(x)\n",
                "print(f\"LandmarkLoRA: {x.shape} -> {y.shape}\")\n",
                "print(f\"LandmarkLoRA params: {sum(p.numel() for p in lm.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load base model (GPT-2 for S²-Attn compatibility)\n",
                "model_name = \"openai-community/gpt2\"\n",
                "base_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "# HyLoRADA Config (Full Features)\n",
                "config = HyLoRADAConfig(\n",
                "    lora_rank=8,\n",
                "    lora_alpha=16.0,\n",
                "    use_dora_magnitude=True,    # DoRA\n",
                "    landmark_enabled=True,      # LandmarkLoRA (Novel)\n",
                "    num_landmarks=8,\n",
                "    position_bias_enabled=True,\n",
                "    # Long Context Features\n",
                "    s2_attn_enabled=True,\n",
                "    s2_group_size=2048,\n",
                ")\n",
                "\n",
                "print(\"Components:\", config.get_component_status())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply HyLoRADA\n",
                "model = HyLoRADAModel(base_model, config)\n",
                "model.print_trainable_params()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Benchmark Section"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Quick Benchmark (GPT-2, 1024 context)\n",
                "# Tests basic integration of LoRA vs HyLoRADA\n",
                "!python run_benchmark.py --model openai-community/gpt2 --methods lora hylorada --epochs 1 --num_train 200 --max_length 1024"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. HyLoRADA Long Context Run (GPT-2, 4096 context)\n",
                "# This is the main experiment for your thesis/project\n",
                "\n",
                "!python run_benchmark.py \\\n",
                "    --dataset longbench \\\n",
                "    --model openai-community/gpt2 \\\n",
                "    --max_length 4096 \\\n",
                "    --methods hylorada \\\n",
                "    --s2_attn \\\n",
                "    --train_embeddings \\\n",
                "    --train_norms \\\n",
                "    --rope_scaling_type linear \\\n",
                "    --rope_scaling_factor 4.0 \\\n",
                "    --epochs 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Full Comparison (All Baselines)\n",
                "# Runs Baseline, LoRA, LoRaDA, LongLoRA, Sparse, and HyLoRADA\n",
                "# WARNING: This may take longer to run\n",
                "\n",
                "!python run_benchmark.py \\\n",
                "    --dataset longbench \\\n",
                "    --model openai-community/gpt2 \\\n",
                "    --max_length 4096 \\\n",
                "    --methods baseline lora lorada longlora sparse hylorada \\\n",
                "    --s2_attn \\\n",
                "    --train_embeddings \\\n",
                "    --train_norms \\\n",
                "    --rope_scaling_type linear \\\n",
                "    --rope_scaling_factor 4.0 \\\n",
                "    --epochs 1"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "gpu",
            "isGpuEnabled": true,
            "isInternetEnabled": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}