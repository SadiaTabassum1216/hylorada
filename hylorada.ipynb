{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# HyLoRADA v0.5.0 - Cost-Efficient Long-Context Learning\n",
                "\n",
                "**Streamlined PEFT for long-context LLMs** based on literature review (2023-2025).\n",
                "\n",
                "Key Techniques:\n",
                "- **S²-Attn (LongLoRA)**: 16x training efficiency\n",
                "- **Trainable Embeddings & Norms**: Critical for >32k context\n",
                "- **RoPE Scaling (YaRN)**: Extend context to 128k+\n",
                "- **Sink Tokens (SinkLoRA)**: Stable attention patterns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repo (Kaggle)\n",
                "import os\n",
                "if os.path.exists('hylorada'):\n",
                "    %cd hylorada\n",
                "    !git pull\n",
                "else:\n",
                "    !git clone https://github.com/SadiaTabassum1216/hylorada.git\n",
                "    %cd hylorada"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q transformers datasets accelerate tqdm bitsandbytes peft"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "import torch\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Quick Start - Standard Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from hylorada import HyLoRADAConfig, HyLoRADAModel\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "# Load model and tokenizer\n",
                "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
                "base_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
                "\n",
                "# Standard Config (efficient, minimal params)\n",
                "config = HyLoRADAConfig(lora_rank=8)\n",
                "model = HyLoRADAModel(base_model, config)\n",
                "\n",
                "print(\"=== Standard Config ===\")\n",
                "model.print_trainable_params()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Long-Context Configuration (>32k Tokens)\n",
                "\n",
                "For efficient long-context learning:\n",
                "- `s2_attn_enabled=True`: S²-Attn for 16x training efficiency\n",
                "- `train_embeddings=True`: Trainable embeddings (LongLoRA)\n",
                "- `s2_sink_tokens=4`: Sink tokens for stable attention (SinkLoRA)\n",
                "- `rope_scaling_type`: Position extension (YaRN)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reload base model for long-context config\n",
                "base_model_long = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
                "\n",
                "# Long-Context Config\n",
                "long_config = HyLoRADAConfig(\n",
                "    lora_rank=8,\n",
                "    # S²-Attn (LongLoRA) - 16x training efficiency\n",
                "    s2_attn_enabled=True,\n",
                "    s2_group_size=2048,\n",
                "    # Trainable embeddings & norms (LongLoRA)\n",
                "    train_embeddings=True,\n",
                "    train_norms=True,\n",
                "    # Sink tokens (SinkLoRA)\n",
                "    s2_sink_tokens=4,\n",
                "    # RoPE Scaling (YaRN)\n",
                "    rope_scaling_type=\"linear\",\n",
                "    rope_scaling_factor=4.0,\n",
                ")\n",
                "\n",
                "model_long = HyLoRADAModel(base_model_long, long_config)\n",
                "\n",
                "print(\"=== Long-Context Config ===\")\n",
                "model_long.print_trainable_params()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run Benchmark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick benchmark: Compare LoRA vs HyLoRADA\n",
                "!python run_benchmark.py --methods lora hylorada --epochs 1 --num_train 200 --num_test 50"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View benchmark results\n",
                "import json, glob\n",
                "\n",
                "for f in glob.glob('./**/benchmark*.json', recursive=True):\n",
                "    print(f\"\\n=== {f} ===\")\n",
                "    try:\n",
                "        data = json.load(open(f))\n",
                "        if 'results' in data:\n",
                "            for method, r in data['results'].items():\n",
                "                ppl = r.get('perplexity', 'N/A')\n",
                "                params = r.get('trainable_params', 'N/A')\n",
                "                time_s = r.get('train_time', 'N/A')\n",
                "                if isinstance(ppl, float):\n",
                "                    ppl = f\"{ppl:.2f}\"\n",
                "                print(f\"{method}: PPL={ppl}, params={params}, time={time_s}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Full Benchmark (All Methods)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Full comparison (takes longer)\n",
                "!python run_benchmark.py --methods lora dora hylorada --epochs 2 --num_train 500 --dataset wikitext"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Unit Tests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run tests to verify installation\n",
                "!python -m pytest tests/ -v --tb=short -q"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "kaggle": {
            "accelerator": "gpu",
            "isGpuEnabled": true,
            "isInternetEnabled": true
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}