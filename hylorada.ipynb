{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# HyLoRADA Unified (v0.4.0)\n",
                "\n",
                "**Parameter-efficient fine-tuning for long-context LLMs.**\n",
                "\n",
                "New Features in v0.4.0:\n",
                "- **LongLoRA**: Trainable Embeddings & Norms\n",
                "- **SinkLoRA**: Sink Token Support\n",
                "- **YaRN**: RoPE Scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repo\n",
                "!git clone https://github.com/SadiaTabassum1216/hylorada.git\n",
                "%cd hylorada"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q transformers datasets accelerate tqdm bitsandbytes peft"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "import torch\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Quick Start\n",
                "Standard HyLoRADA configuration for general fine-tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from hylorada import HyLoRADAConfig, HyLoRADAModel\n",
                "from transformers import AutoModelForCausalLM\n",
                "\n",
                "# Load a small model for demonstration\n",
                "try:\n",
                "    base_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\", trust_remote_code=True)\n",
                "    \n",
                "    # Standard Config (Efficiency)\n",
                "    config = HyLoRADAConfig(lora_rank=8)\n",
                "    model = HyLoRADAModel(base_model, config)\n",
                "    \n",
                "    print(\"\\n=== Standard Config ===\")\n",
                "    model.print_trainable_params()\n",
                "except Exception as e:\n",
                "    print(f\"Model loading skipped (Internet required): {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Long-Context Configuration (>32k Tokens)\n",
                "Enable advanced features for long-context learning:\n",
                "- `train_embeddings=True`: Unlock embedding layer training (LongLoRA/LongQLoRA)\n",
                "- `s2_sink_tokens=4`: Enable sink tokens for stable attention (SinkLoRA)\n",
                "- `rope_scaling_type=\"linear\"`: Apply positional interpolation (YaRN)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Long-Context Config\n",
                "long_context_config = HyLoRADAConfig(\n",
                "    lora_rank=8,\n",
                "    # LongLoRA\n",
                "    train_embeddings=True,  \n",
                "    train_norms=True,\n",
                "    # SinkLoRA\n",
                "    s2_attn_enabled=True,\n",
                "    s2_sink_tokens=4,\n",
                "    # YaRN / LongRoPE\n",
                "    rope_scaling_type=\"linear\",\n",
                "    rope_scaling_factor=2.0\n",
                ")\n",
                "\n",
                "try:\n",
                "    model_long = HyLoRADAModel(base_model, long_context_config)\n",
                "    \n",
                "    print(\"\\n=== Long-Context Config ===\")\n",
                "    model_long.print_trainable_params()\n",
                "except NameError:\n",
                "    pass # base_model not loaded"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run Benchmark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run benchmark comparison\n",
                "!python run_benchmark.py --methods lora hylorada --epochs 1 --num_train 200"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze results\n",
                "import json, os, glob\n",
                "\n",
                "for f in glob.glob('./**/benchmark*.json', recursive=True):\n",
                "    print(f\"\\n=== {f} ===\")\n",
                "    try:\n",
                "        data = json.load(open(f))\n",
                "        if 'results' in data:\n",
                "            for m, r in data['results'].items():\n",
                "                metric = 'perplexity' if 'perplexity' in r else 'accuracy'\n",
                "                val = r.get(metric, 'N/A')\n",
                "                if isinstance(val, float):\n",
                "                    val = f\"{val:.2f}\"\n",
                "                print(f\"{m}: {metric}={val}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error reading {f}: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "kaggle": {
            "accelerator": "gpu",
            "isGpuEnabled": true,
            "isInternetEnabled": true
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}